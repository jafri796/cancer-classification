training:
  batch_size: 32
  epochs: 50
  optimizer:
    type: adamw
    lr: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8
  lr_scheduler:
    type: cosine
    eta_min: 1e-7
    warmup_epochs: 2
  loss:
    type: label_smoothing_bce
    label_smoothing: 0.1
  mixed_precision:
    enabled: true
  gradient_clipping:
    enabled: true
    max_norm: 1.0
  accumulation_steps: 1
  early_stopping:
    patience: 15
    min_delta: 0.0001
  seed: 42